{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder_dynamic.ipynb\n",
    "# Copyright 2020 André Carrington, Ottawa Hospital Research Institute\n",
    "# Use is subject to the Apache 2.0 License\n",
    "# Written by André Carrington\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# suppress warnings because they are verbose and distracting\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cycle_names    = [\n",
    "  'cchs-82M0013-E-2001-c1-1-general-file_F1.csv',\n",
    "  'cchs-82M0013-E-2003-c2-1-GeneralFile_F1.csv',\n",
    "  'cchs-82M0013-E-2005-c3-1-main-file_F1.csv',\n",
    "  'CCHS-82M0013-E-2009-2010-Annualcomponent_F1.csv',\n",
    "  'CCHS-82M0013-E-2010-AnnualComponent_F1.csv',\n",
    "  'cchs-82M0013-E-2011-2012-Annual-component_F1.csv',\n",
    "  'cchs-82M0013-E-2012-Annual-component_F1.csv',\n",
    "  'cchs-82M0013-E-2013-2014-Annual-component_F1.csv',\n",
    "  'cchs-82M0013-E-2014-Annual-component_F1.csv',\n",
    "  'cchs-E-2007-2008-AnnualComponent_F1.csv'\n",
    "];\n",
    "\n",
    "test_run_text     = input('experiment number: ')\n",
    "test_cycles_text  = input('test_cycles (e.g., 1): ')\n",
    "encoding_dim_text = input('encoded_dimensionality (e.g., 50): ')\n",
    "encoded_mult_text = input('encoded_multiples (e.g., 0,8,4,1; where 0 is dropout): ') # [8 4 1]\n",
    "main_activ        = input('main activation function (e.g., relu, sigmoid, tanh): ')\n",
    "output_activ      = input('output activation function (e.g., sigmoid, tanh, relu): ')\n",
    "dropout_rate_text = input('dropout rate if applicable (e.g., 0.2): ')\n",
    "train_noise_text  = input('training noise (size of middle 4 standard deviations, e.g., 0 or 0.3): ')\n",
    "val_noise_text    = input('validation noise (size of middle 4 standard deviations, e.g., 0 or 0.3): ')\n",
    "optimizer         = input('optimizer (e.g., Nadam, Adam, Adamax, Adadelta, Adagrad, RMSprop, SGD): ')\n",
    "objective         = input('objective to minimize (e.g., MSE, MAE): ')\n",
    "max_epochs_text   = input('max_epochs (e.g., 400): ')\n",
    "patience_text     = input('patience (e.g., 15): ')\n",
    "\n",
    "tic               = time.perf_counter()\n",
    "\n",
    "encoding_dim      = int(encoding_dim_text)\n",
    "test_cycles       = [int(i) for i in list(test_cycles_text.split(\",\"))] \n",
    "encoded_multiples = [int(i) for i in list(encoded_mult_text.split(\",\"))] \n",
    "dropout_rate      = float(dropout_rate_text)\n",
    "train_noise       = float(train_noise_text)\n",
    "val_noise         = float(val_noise_text)\n",
    "# optimizer\n",
    "# objective\n",
    "max_epochs        = int(max_epochs_text)\n",
    "patience_val      = int(patience_text)\n",
    "\n",
    "# based on the metric we choose to optimize\n",
    "# get a list of the other metrics (into mets) to also report on\n",
    "mets = []\n",
    "for met in ['MSE','MAE','binary_crossentropy','cosine_similarity']:\n",
    "    if met == objective:\n",
    "        continue\n",
    "    mets = mets + [met]\n",
    "#endfor\n",
    "\n",
    "#test_cycles      = list(test_cycles_text.split(\",\"))\n",
    "#encoded_multiples= range(2,19)/2 # (2 to 18)/2 = 1,1.5,2,...,8.5,9\n",
    "\n",
    "# Import data into a Pandas Dataframe (df)\n",
    "temp_df        = {};\n",
    "in_path        = 'C:\\\\Users\\\\amcarrin\\\\Desktop\\\\Andre2\\\\PUMF_data\\\\'\n",
    "#in_path= 'PUMF_data/';\n",
    "#in_path= '//Users/andrecarrington/Documents/0 OHRI/Projects/Montfort CDSS/PUMF/';\n",
    "for i in range(0,len(test_cycles)):\n",
    "    #in_fn     = in_path + cycle_names[int(test_cycles[i])-1]\n",
    "    in_fn      = in_path + cycle_names[test_cycles[i]-1]\n",
    "    print       (in_fn)\n",
    "    temp_df[i] = pd.read_csv(in_fn)\n",
    "    print(temp_df[i].shape)\n",
    "    \n",
    "toc            = time.perf_counter()\n",
    "print(f\"Loaded files in {toc - tic:0.1f} seconds\")\n",
    "\n",
    "# only use the first cycle for now\n",
    "in_df   = temp_df[0]\n",
    "\n",
    "# drop the administrative record (sequence) number\n",
    "in_df = in_df.drop(columns=['ADMA_RNO'])\n",
    "\n",
    "# # load a dataframe with a column listing the relevant variables \n",
    "# temp_df  = pd.read_csv(\"relevant_variables.csv\",header=None)\n",
    "# rel_vars = temp_df.iloc[:,0] # convert df to series\n",
    "# temp_df  = in_df.copy()\n",
    "# in_df    = temp_df[rel_vars] # filter in_df to contain only relevant variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print(f\"Imported tensorflow and keras in {toc - tic:0.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu(x, alpha=0.01): \n",
    "    if (x>0):\n",
    "        return np.maximum(0, x)\n",
    "    else:\n",
    "        return x*alpha\n",
    "    #endif\n",
    "#enddef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic               = time.perf_counter()\n",
    "\n",
    "# _dim for dimensions\n",
    "in_feature_dim    = in_df.shape[1] # 613 \n",
    "in_instance_dim   = in_df.shape[0] # 130880\n",
    "\n",
    "# Example of a 1+1 layer autoencoder, where xi,yi are defined in code further below\n",
    "# x = Dense(encoding_dim,   activation='relu'   )(xi) # encoder\n",
    "# a = Dense(in_feature_dim, activation='sigmoid')(x)  # autoencoder\n",
    "# y = Dense(in_feature_dim, activation='sigmoid')(yi) # decoder\n",
    "\n",
    "# Example of a 2+2 layer autoencoder, where xi,yi are defined in code further below\n",
    "# x = Dense(2*encoding_dim, activation='relu'   )(xi)\n",
    "# x = Dense(  encoding_dim, activation='relu'   )(x)  # encoder\n",
    "# a = Dense(2*encoding_dim, activation='relu'   )(x)\n",
    "# a = Dense(in_feature_dim, activation='sigmoid')(a)  # autoencoder\n",
    "# y = Dense(2*encoding_dim, activation='relu'   )(yi)\n",
    "# y = Dense(in_feature_dim, activation='sigmoid')(y)  # decoder\n",
    "\n",
    "# check encoded_multiples, e.g., [8,4,1]\n",
    "if len(encoded_multiples)==0:\n",
    "    raise ValueError('At least one multiple is required')\n",
    "if encoded_multiples[-1] != 1:\n",
    "    raise ValueError('The last multiple should be 1')\n",
    "    \n",
    "# Build the encoder x\n",
    "xi          = Input(shape=(in_feature_dim,)) # input to encoder x and autoencoder a\n",
    "x           = {}\n",
    "last        = len(encoded_multiples)-1\n",
    "for m in range(last+1):\n",
    "    mult = encoded_multiples[m]\n",
    "    if m == 0: # first_layer\n",
    "        layer_input = xi\n",
    "    else:\n",
    "        layer_input = x\n",
    "    #endif\n",
    "    if   mult == 0:   # indicates a dropout layer\n",
    "        x   = Dropout(dropout_rate)(layer_input)\n",
    "    elif mult >  0:   # multiples greater than 0 indicate a dense layer\n",
    "        x   = Dense(mult*encoding_dim, activation=main_activ)(layer_input)\n",
    "    #endif\n",
    "#endfor\n",
    "    \n",
    "encoder_m   = Model(xi, x)\n",
    "    \n",
    "# Build the decoder y and autoencoder a\n",
    "decoded_multiples = encoded_multiples.copy()   # [0,8,4,1]\n",
    "if decoded_multiples[0] == 0:                  # for decoder, do not \"relect\" the encoder's first dropout layer\n",
    "    decoded_multiples = decoded_multiples[1:]  # [8,4,1] \n",
    "decoded_multiples.reverse()                    # [1,4,8] reflect/reverse. note: a last layer will be automatically added \n",
    "decoded_multiples = decoded_multiples[1:]      # [4,8]   note: this array can be zero length\n",
    "yi          = Input(shape=(encoding_dim,))     # input to decoder y\n",
    "y           = {}\n",
    "a           = {}\n",
    "last        = len(decoded_multiples)-1\n",
    "#last        = len(decoded_multiples)\n",
    "if len(decoded_multiples)==0: # single layer decoder\n",
    "    y       = Dense(in_feature_dim, activation=output_activ)(yi)\n",
    "    a       = Dense(in_feature_dim, activation=output_activ)(x)    \n",
    "else:\n",
    "    for m in range(last+1):\n",
    "        mult = decoded_multiples[m]\n",
    "        if m == 0: # first_layer\n",
    "            layer_input  = yi\n",
    "            layer_inputa = x\n",
    "        else:\n",
    "            layer_input  = y\n",
    "            layer_inputa = a\n",
    "        #endif            \n",
    "        if   mult == 0:   # indicates a dropout layer\n",
    "            y   = Dropout(dropout_rate)(layer_input)\n",
    "            a   = Dropout(dropout_rate)(layer_inputa)\n",
    "        elif mult >  0:   # multiples greater than 0 indicate a dense layer\n",
    "            # add layer\n",
    "            y = Dense(mult*encoding_dim, activation=main_activ)(layer_input)\n",
    "            a = Dense(mult*encoding_dim, activation=main_activ)(layer_inputa)\n",
    "            if m  == last: # after last defined layer, add a final output layer too, automatically\n",
    "                y = Dense(in_feature_dim, activation=output_activ)(y)\n",
    "                a = Dense(in_feature_dim, activation=output_activ)(a)\n",
    "            #endif\n",
    "        #endif\n",
    "    #endfor\n",
    "#endif\n",
    "\n",
    "decoder_m     = Model(yi, y)\n",
    "autoencoder_m = Model(xi, a)\n",
    "\n",
    "autoencoder_m.compile(optimizer=optimizer, loss=objective, metrics=mets)\n",
    "# autoencoder_m.compile(optimizer='Nadam', \n",
    "#                       loss='MSE',\n",
    "#                       metrics=['cosine_similarity','MAE','binary_crossentropy'])\n",
    "# autoencoder_m.compile(optimizer='adadelta', \n",
    "#                      loss='MSE',\n",
    "#                      metrics=['cosine_similarity','MAE','binary_crossentropy'])\n",
    "\n",
    "toc           = time.perf_counter()\n",
    "print(f\"Compiled models in {toc - tic:0.1f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autoencoder_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing   import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "\n",
    "tic                 = time.perf_counter()\n",
    "\n",
    "# \"normalize\" to positive range using min-max standardization to [0,1]\n",
    "# result is a numpy.ndarray (nd), convert it back to a Pandas dataframe (df)\n",
    "temp_df             = in_df.copy()\n",
    "scaler              = MinMaxScaler()  # obtain the scaler for later denormalization\n",
    "in_std_nd           = scaler.fit_transform(temp_df)\n",
    "in_std_df           = pd.DataFrame(in_std_nd)\n",
    "\n",
    "toc                 = time.perf_counter()\n",
    "print(f\"Normalized data in {toc - tic:0.1f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic           = time.perf_counter()\n",
    "\n",
    "if train_noise == 0:\n",
    "    in_noi_df   = in_std_df\n",
    "else:\n",
    "    in_noi_df   = in_std_df + \\\n",
    "                train_noise * np.random.normal(loc=0.0, scale=1/4, size=in_std_df.shape)\n",
    "    fix_max_index = in_noi_df > 1\n",
    "    fix_min_index = in_noi_df < 0\n",
    "    in_noi_df[fix_max_index]  = 1\n",
    "    in_noi_df[fix_min_index]  = 0\n",
    "\n",
    "if   val_noise == train_noise:\n",
    "    val_noi_df = in_noi_df.copy()\n",
    "elif val_noise == 0:\n",
    "    val_noi_df = in_std_df.copy()\n",
    "else:\n",
    "    val_noi_df = in_std_df + \\\n",
    "                 val_noise * np.random.normal(loc=0.0, scale=1/4, size=in_std_df.shape)\n",
    "    fix_max_index = val_noi_df > 1\n",
    "    fix_min_index = val_noi_df < 0\n",
    "    val_noi_df[fix_max_index]  = 1\n",
    "    val_noi_df[fix_min_index]  = 0\n",
    "    \n",
    "toc           = time.perf_counter()\n",
    "print(f\"Created noisy (standardized) data in {toc - tic:0.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic                 = time.perf_counter()\n",
    "\n",
    "# split data into 70% training, 20% validation 10% hold out testing\n",
    "# use KFold to get 10 mutually exclusive folds with shuffling\n",
    "# note: not being used for cross-validation (at this time) -- just for train / validation / test split\n",
    "kf=KFold(n_splits=10, random_state=26, shuffle=True)\n",
    "\n",
    "i = 0\n",
    "train_index = list()\n",
    "val_index   = list()\n",
    "test_index  = list()\n",
    "for unused, val_temp in kf.split(in_df):\n",
    "    print(val_temp)\n",
    "    i = i + 1\n",
    "    if i < 9:\n",
    "        train_index.extend(val_temp.tolist())\n",
    "    if i==8 or i==9:\n",
    "        val_index.extend(val_temp.tolist())\n",
    "    if i==10:\n",
    "        test_index = val_temp.tolist()\n",
    "\n",
    "print(len(train_index))\n",
    "\n",
    "train_in_df     =  in_df.iloc[train_index,:]\n",
    "val_in_df       =  in_df.iloc[val_index  ,:]\n",
    "test_in_df      =  in_df.iloc[test_index ,:]\n",
    "    \n",
    "train_in_std_df =  in_std_df.iloc[train_index,:]\n",
    "val_in_std_df   =  in_std_df.iloc[val_index  ,:]\n",
    "test_in_std_df  =  in_std_df.iloc[test_index ,:]\n",
    "    \n",
    "train_in_noi_df =  in_noi_df.iloc[train_index,:]\n",
    "val_in_noi_df   = val_noi_df.iloc[val_index  ,:]\n",
    "test_in_noi_df  =  in_noi_df.iloc[test_index ,:]\n",
    "    \n",
    "toc                 = time.perf_counter()\n",
    "print(f\"Shuffled and split data in {toc - tic:0.1f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic                 = time.perf_counter()\n",
    "\n",
    "out_fn              = in_path + 'val_in_df_' + test_run_text + '.csv'\n",
    "val_in_df.to_csv(out_fn, index = False, header=True)\n",
    "\n",
    "toc                 = time.perf_counter()\n",
    "print(f\"Wrote non-standardized data to file in {toc - tic:0.1f} seconds\")\n",
    "\n",
    "# show the non standardized validation set \n",
    "# to compare against the decoder result that comes later\n",
    "val_in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the standardized validation set \n",
    "# to compare against the decoder result that comes later\n",
    "val_in_std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# learn the autoencoder on the standardized training data, validated with standardized validation data\n",
    "\n",
    "tic            = time.perf_counter()\n",
    "\n",
    "# prevent overfitting with EarlyStopping (es) at minimum validation loss, \n",
    "# allowing for patience=15 (for example) epochs of no better value \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es             = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience_val)\n",
    "\n",
    "# run the autoencoder with a callback to es, and capturing history _h\n",
    "loss_metric_h  = autoencoder_m.fit(train_in_noi_df, train_in_std_df, epochs=max_epochs, batch_size=256,\n",
    "                                   shuffle=True, validation_data=(val_in_noi_df, val_in_std_df),\n",
    "                                   callbacks=[es])\n",
    "\n",
    "toc            = time.perf_counter()\n",
    "print(f\"Trained and validated autoencoder in {toc - tic:0.1f} seconds\")\n",
    "\n",
    "tic            = time.perf_counter()\n",
    "\n",
    "# convert the history dict to a pandas DataFrame:     \n",
    "loss_metric_df = pd.DataFrame(loss_metric_h.history) \n",
    "\n",
    "# save to csv: \n",
    "hist_csv_file  = in_path + 'loss_metric_history_' + test_run_text + '.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    loss_metric_df.to_csv(f)\n",
    "\n",
    "toc            = time.perf_counter()\n",
    "print(f\"Wrote loss_metric data to file in {toc - tic:0.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic             = time.perf_counter()\n",
    "\n",
    "# encode and decode data using the validation set\n",
    "encoded_imgs_nd = encoder_m.predict(val_in_std_df)\n",
    "decoded_imgs_nd = decoder_m.predict(encoded_imgs_nd)\n",
    "print('encoded_imgs_nd: ',type(encoded_imgs_nd),encoded_imgs_nd.shape)\n",
    "print('decoded_imgs_nd: ',type(decoded_imgs_nd),decoded_imgs_nd.shape)\n",
    "\n",
    "toc             = time.perf_counter()\n",
    "print(f\"Encoded and decoded data in {toc - tic:0.1f} seconds\")\n",
    "\n",
    "# show the standardized output, val_out_std_df\n",
    "val_out_std_df  = pd.DataFrame(decoded_imgs_nd)\n",
    "val_out_std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic             = time.perf_counter()\n",
    "\n",
    "# get and show the output (removing the standardization)\n",
    "val_out_nd      = scaler.inverse_transform(decoded_imgs_nd)\n",
    "val_out_df      = pd.DataFrame(val_out_nd)\n",
    "\n",
    "out_fn          = in_path + 'val_out_df_' + test_run_text + '.csv'\n",
    "val_out_df.to_csv(out_fn, index = False, header=True)\n",
    "\n",
    "toc             = time.perf_counter()\n",
    "print(f\"Denormalized and saved data in {toc - tic:0.1f} seconds\")\n",
    "\n",
    "val_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
